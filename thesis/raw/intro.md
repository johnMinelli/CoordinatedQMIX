---
title: "Thesis"
output:
  pdf_document:
    path: intro.pdf
    citation_package: natbib
    latex_engine: xelatex
fontsize: 11pt
geometry: margin=1in
---

<!-- %Help%
- State the problem, your approach and solution, and the main contributions of the paper.
- Include little if any background and motivation.
- Be factual but comprehensive. Note that The material in the abstract should not be repeated later word for word in the paper.
-->
# Abstract
<!-- Reflecting on the unnecessity of over complicated communication and neither of external locking schemes for coordinated actions we argue that presenting agents in front of the opportunity to act selfishly or collaborate they will choose for their best with eventually the emergence of proper coordination if it would be the case. -->
<!-- by investigating in coordinated methods of achieving shared consensus and optimal action selection for coordinated behaviour through local information and communication with neighbouring agents. TODO (differently from others) ... the proposed approach emphasises the importance of considering each neighbouring agent's intentions in relation to personal objectives in developing a coordinated action plan. TODO (results show that). -->

Coordination of actions plays a crucial role in multi-agent systems as it allows entities to work together towards a common goal or individually in a shared environment without hindering each other's progress. In order for this to occur, it is essential for them to be able to understand and acknowledge each other's intentions. But achieving this level of spatial awareness and collaborative skill is a complex task, as real-world constraints such as limited knowledge acquisition, computation requirements, and communications with neighbouring agents must be taken into consideration. This thesis aims to contribute to the research field by studying coordination among agents habilitated to exchange information. The challenges and existing solutions are discussed, then an alternative view for solving the problem is presented. The paper argues that mechanisms to force coordination are unnecessary since agents that can freely choose whether to collaborate or not can be more proficient in general situations and still occasionally work together when it results in the best option. In support of this claim, CoMix, a novel architecture that reflects this strategy is proposed and evaluated on a large scale through thorough testing, showing beneficial results and strategic behaviours from agents in action.


<!-- %Help%
- What is the problem? Why is it interesting and important?
- Why is it hard? (E.g., why do naive approaches fail?)
- Why hasn't it been solved before? (Or, what's wrong with previous proposed solutions? How does mine differ?)
- What are the key components of my approach and results? Also include any specific limitations.
-->
# Introduction
<!-- We will explore how to achieve shared consensus in action selection through only local computation and communication with neighbouring agents. -->
<!-- While cooperation and competition are commonly studied behaviours in multi-agent test environments, it is important to recognise that most real-world scenarios do not fit neatly into a defined category that encompasses both. Instead, the goal of coordination between entities in a shared space is more comprehensive, enabling optimal system evolution regardless of individual objectives. --> 

Cooperation and competition are commonly studied behaviours, with state-of-the-art methods excelling in their respective task [1,2], but, it is important to recognise that most real-world scenarios do not fit neatly into a defined category that encompasses both. Instead, by focusing on coordination dynamics, it would be possible to develop methods enabling optimal system evolution regardless of individual objectives set by the environment [11]. This will require tackling the difficult challenges of information sharing and decentralised decision-making, but developments in this direction could lead to significant advances in technology applications, especially in areas involving social skills such as swarm robotics and close interactions such as autonomous vehicle navigation.


## Multi-Agent Systems
A Multi-agent System (MAS) encompasses multiple autonomous entities, referred to as agents, that interact with each other within a shared environment [21]. The goal of each agent is to accomplish a specific task, and the behaviour required to do so can be quite complex, possibly requiring interactions of cooperative or competitive type, depending on the task at hand. Due to the complexity of such systems, instead of developing intelligent behaviours from scratch is possible to inject intelligence into the agents, pre-programming responses to interactions, or adopt fixed shared rules which decrease the space of uncertainty. However, it is generally more desirable for agents to possess the ability to adapt and learn over time. One prominent framework for that learning ability is Reinforcement Learning (RL), which entails modifying behaviour through a process of trial and error.  
Recent advances in Artificial Intelligence (AI) and Deep Learning (DL) have led to a surge of research interest in Multi-agent Reinforcement Learning (MARL). This approach has demonstrated success in a wide range of fields, including robotics, natural language processing, game playing, and network security. In particular, RL has been utilised to develop intelligent robots that can navigate and manipulate objects in their environment [11p]; strategy selections for acting in impractical spaces explorations [12p] or optimisation of resources [13p]; in classic game playing it has been used to develop agents that can compete at human levels, such as playing chess, Go and poker [14p,15p,16p,17p], or in the case of strategic multiplayer online games where cooperation with other agents is required to achieve a common goal, such as DOTA 2 [18p] and StarCraft II [19p].
Despite the impressive results that can be achieved with deep reinforcement learning, its application to a multi-agent setting poses unique challenges: the concurrent and heterogeneous behaviour of the agents leads to an unpredictable environment, referred to as non-stationarity [2p,5p,10p]; the exponential explosion of states leads to the curse of dimensionality [2p,5p], making it difficult to assign credit to specific agents for a given outcome [31p,32p]; large action spaces coupled with the need for global exploration [8p], increasing the complexity of the learning process; the potential for relative overgeneralisation [33p,34p,35p]. For this reason, a successful single-agent RL methodology cannot be simply applied to a multi-agent setting without re-evaluating its learning approach.

## Coordination
Intelligent coordination refers to the coordinated effort of a group of agents who are able to make intelligent decisions to act not only on the basis of their own goals but taking into consideration other entities in the environment as well. It involves establishing a shared understanding of the task at hand and developing a plan that outlines the roles and responsibilities of each agent. This concept is a promising area of research in MARL as it addresses the challenges introduced by the presence of multiple entities, enabling more effective action selection and limiting inefficient behaviours.
In general, we seek coordination ability both in cooperative and competitive settings.

- Cooperation refers to the act of working together towards a common goal. It involves the mutual support and assistance of multiple agents, each of whom contributes their own unique skills and abilities to the collective effort.
- Competition refers to the ability of agents to compete with each other to achieve their goals. It typically involves agents taking actions that maximise their own reward or utility, potentially at the expense of other agents.

Cooperative contexts are undoubtedly the most researched [3] since their successful application among agents can lead to the achievement of greater goals beyond the capabilities of the single agent and the emergence of group intelligence. This is the foundation of many applications in robotics, swarm intelligence, and social studies. However, competition is also a vital aspect worth considering, as it can motivate agents to improve their performance and explore the environment more efficiently. Furthermore, competition can prevent agents from becoming too dependent on one another and encourage them to develop more sophisticated strategies. Despite this common distinction, it should be noted that usually, we do not have a clear cut between cooperative and competitive behaviour in most real-world scenarios. [4,5] have pointed out that a cooperative agent may temporarily act selfishly while trying to achieve a common goal, and a competitive agent may temporarily form a coalition with its opponent to achieve its own goal. Therefore, when designing a coordination system, it is important to avoid injecting a fixed criterion for collaboration or obstruction.

## Information Sharing
Intelligent agents (whether humans or artificial) can greatly benefit from the ability of information exchange to coordinate, strategise, and combine reciprocally their sensory experiences to act in the environment. Indeed, it is usually assumed that agents placed in the real world have to operate in situations of partial observability, limited in their knowledge and perception of their surroundings, and it would be unrealistic to assume otherwise.
By enabling communication, we can aid the agent in gathering information about the environment and improve its decision-making process by sharing observations, action policies, future intentions, or other relevant information. Furthermore, communication enables agents to form strong relationships and work together in groups, leading to improved behaviours and increased efficiency in task completion. This is achieved through the parallelisation of activities and optimisation of resources where local neighbourhoods allow reductions in the number of messages exchanged and computations wasted. We can see examples of this ability in a wide range of RL applications, like multi-player gameplay in simulated game environments (e.g., DoTA, StarCraft) or physical worlds (e.g., robot soccer), self-driving car networks working together for safe and efficient transportation or teams of robots deployed in hostile and rapidly-evolving environments, as well as many others.

## Centralised Decision
When trying to achieve coordinated behaviour, a straightforward approach is to adopt a hierarchical structure, where one agent takes on the role of coordinator to establish order and effectiveness in decisions. However, the question arises of who should be in charge of this role and who better understands the situation. Centralised coordination approaches delegate the responsibility of coordinating the agents to a single agreed-upon entity, but even if a reliable hierarchical mechanism is in place, the question remains: How can the higher-level agent acquire the necessary information and successfully coordinate all other agents? How can this method address scalability and generalisation to different situations?
An alternative and often preferred approach is a decentralised one, where there are no agents with higher roles controlling the behaviour of others. This method eliminates the difficulties associated with centralisation, such as global coordination and scalability issues. On the other hand, agents in a decentralised system have limited knowledge and must rely solely on their local information, which increases uncertainty and variability of action and makes it difficult to predict the overall behaviour of the group. As a result, this leads to suboptimal policies and ineffective interactions during testing, especially when complex coordination is required. In particular, the problem has been widely studied under the name of Byzantine generals, or interactive consistency, where the agents have to take a group action with shared consensus to succeed.

## From Simulation to Reality
In the field of RL research, there is often a strong desire to achieve realism and, when possible, to deploy agents in real-world settings. However, using black box algorithms such as deep neural networks, for decision-making, can make it difficult to transfer these agents from simulated environments to the real world in laboratory or production settings because of a lack of explainability and predictability and because of the gap between a training and testing environment. To address this difficulty, there has been a trend towards using more realistic simulation environments and combining RL and robotic research efforts. Thanks also to independent organisations [6] and open source communities [7], many simulated environments are available to test physics interactions, structural properties, and complex behaviour at scale. In this thesis, the aim is also to gain good insights into the proposed algorithm's performance in a more realistic environment.

It is also worth mentioning that the communication process plays a key role in RL. There is a vast research activity in the telecommunication field and under the Internet of Things (IoT) umbrella that tries to design infrastructure and cope with interconnected entities limited in their computation and decision ability. However, this thesis will not cover the physical aspects that agents deployed in real scenarios are usually subject to. The analysis and reasoning will assume an agent-to-agent (A2A) communication, following the traditional device-to-device (D2D) paradigm, where physically close devices (e.g., two agents), can communicate directly over a so-called sidelink. Compared to regular centralised uplink-downlink communication, D2D communications benefit from a shorter link distance and fewer hops, which is better in terms of reliability. Moreover, since communication is direct, i.e., without intermediate nodes, D2D has the potential to provide lower latency in the transmission of information.

<!-- This research contributes to the field of AI by addressing the challenges of coordination and communication in multi-agent systems, which are crucial for the extensive use of AI in the wild.-->
These topics represent central points of the research field, and developments in their directions are crucial for the extensive use of AI in the wild. The central focus of this thesis work is the development of new architectures and strategies for autonomous agents operating in shared environments. Specifically, the research aims to investigate the ability of agents to learn and coordinate autonomously when communication and coordination are necessary. The proposed architecture and strategies aim to answer the following research questions:

- Can agents acting in a shared environment with the ability to communicate, learn autonomously when coordination is necessary and preferable to selfish behaviour?
- Is it possible to design an action strategy for independent agents that uses simple communication to achieve effective group coordination?

The aim, therefore, is to demonstrate, under different constraints and needs, whether a consensus in behaviour can be reached through the individual striving for a better reward without forcing coordination.

# Background
This chapter provides a comprehensive introduction to reinforcement learning and multi-agent reinforcement learning. Beginning with a naturalistic explanation of the mechanisms involved, it proceeds by discussing the taxonomy of problems and tractability properties. The focus then shifts to the area of communication and coordination mechanisms, which forms the core of this thesis project.

## Origins and Definition of Reinforcement Learning
<!--Historical perspective-->
The field of RL has its origins in the study of animal learning and behaviour, specifically in the psychological literature [8] and animal experimentation [9], which have shown that animals can learn to perform complex tasks through trial-and-error, with the help of rewards and punishments. Later, the concept of RL was formalised in the field of artificial intelligence, where it is defined as a type of learning in which an agent learns to perform actions that maximise a scalar reward signal. The agent's goal is to learn a policy, which is a mapping from states of the environment to actions, that maximises the expected cumulative reward over time. [10].

<!--Markov decision process-->
The standard mathematical framework for modelling sequential decision-making problems is the Markov decision process (MDP), which is defined as a tuple $M = (S, A, P, r, \gamma)$, where:

- $S$ is the state-space, a finite set of world states, represented as $S = {1, 2, \dots, |S|}$.
- $A$ is the action-space, a finite set of actions, represented as $A = {1, 2, \dots, |A|}$.
- $P(s'|s, a)$ is the state transition probability function that expresses the probability of transitioning from state $s$ to state $s'$ by selecting action $a$.
- $r = R(s, a, s')$ is the reward obtained from the reward function, given the transition from state $s$ to state $s'$ by taking action $a$.
- $\gamma \in [0, 1]$ is the discount factor, which is used to handle both finite and infinite-horizon problems.

The goal of an RL agent in an MDP is to find a deterministic, optimal policy $\pi^*: S \rightarrow A$, which will dictate how the agent should act in order to maximise its rewards. Mathematically, the optimal policy can be defined as
$$\pi^* = \arg \max_{\pi \in \theta} \mathbb{E}[\sum_{t=0}^{\infty} \gamma^t R(s_t, \pi(s_t), s_{t+1}) \mid s_0 = s]$$
where $\theta$ is the set of all admissible deterministic policies, and $(s_0, a_0, s_1, a_1, \dots)$ is a state-action trajectory generated by the Markov chain under policy $\pi$. The optimal policy is the one that maximises the expected cumulative reward over an infinite horizon.

<!--Resolution methods through estimation of V/Q-->
An alternative to directly searching for the optimal policy is to define two utility functions that capture the concept of expected return. These are the value function and the state-action value function, also known as the Q-function. The value function for a given policy $\pi$ is defined as:
$V^{\pi}(s) = E [\sum_{k=0}^{\infty} \gamma^kr(s_k, \pi(s_k))|s_0 = s]$
It encodes the expected cumulative reward when starting in state $s$ and following the policy $\pi$ thereafter. The state-action value function, or Q-function, is defined as:
$Q^{\pi}(s, a) = E [\sum_{k=0}^{\infty} \gamma^kr(s_k, \pi(s_k))|s_0 = s, a_0 = a]$
It measures the expected cumulative reward when starting from state $s$, taking action $a$, and then following the policy $\pi$. In the context of deep reinforcement learning, the policy, the value functions, or both are typically represented by neural networks. [11]

## Multi Agent Reinforcement Learning
<!--MARL as MG-->
In multi-agent reinforcement learning, we extend the single-agent case by introducing a different formulation known as a Markov Game. This is a generalisation of Markov Decision Processes and allows for modelling more complex decision-making scenarios where agents need to make strategic decisions based on the actions of other agents. In a Markov Game, each agent acts according to its own policy, which may differ from one agent to another, affecting each other's rewards and future states. This is formalized by the tuple $(N, S, {A_i}, P, {R_i}, \gamma)$, where:

- $N = {1, â€¦, N}$ denotes the set of $N > 1$ interacting agents
- $S$ is the set of states observed by all agents
- $A = A_1 \times \cdots \times A_N$ joint action space is the collection of individual action spaces from agents $i \in N$
- $P: S \times A \rightarrow P(S)$ is the transition probability function and describes the chance of a state transition
- $R_i$ is the reward function defined as $R_i: S \times A \times S \rightarrow \mathbb{R}$ associated to each agent $i \in N$
- $\gamma \in [0,1]$ is the discount factor

At stage $t$, each agent $i \in N$ selects and executes an action based on the individual policy $\pi_i: S \rightarrow P(A_i)$. The system evolves from state $\textbf{s}$ under the joint action $\textbf{a}$ with respect to the transition probability function $P$ to the next state $\textbf{s}'$, while each agent receives $\textbf{r}$ as immediate feedback to the state transition. The goal of each agent, similar to a single-agent problem, is to modify its policy in order to maximise its long-term rewards  [12].

##  Multi-Agent Systems Taxonomy
<!-- Coop/Coord/Mixed -->
The study of MAS often involves categorising situations using standard taxonomies to understand the system's characteristics clearly, compare it with other multi-agent systems, and identify specific challenges and opportunities. For example, we can distinguish between settings where the rewards obtained are shared or individually assigned. The relative taxonomy classifies them as:

- **Fully cooperative** setting, in which all agents receive the same reward for state transitions, i.e. $R = R_i = \dots = R_N$. Agents are motivated to collaborate in order to maximise the performance of the team.
- **Fully competitive** setting, where the problem is described as a zero-sum Markov Game. In this setting, the sum of rewards equals zero for any state transition, i.e. $R = \sum_{i=1}^N R_i(s, a, s') = 0$. Agents are motivated to maximise their own individual reward while minimising the reward of others.
- **Mixed** setting, also known as a general-sum game, the setting is neither fully cooperative nor fully competitive, and therefore does not impose any restrictions on the goals of the agents.

<!-- Centr/Decentr -->
Another commonly used taxonomy regards the learning and execution process, where we have a distinction based on the information available to agents.

- **Decentralised** settings [13,14], are characterised by the presence of independent learners who are unaware of the existence of other agents and are unable to observe their rewards or actions. A lack of global observability and coordination among agents usually marks this type of setting.
**Centralised** settings [15,16], feature joint-action learners capable of observing the actions taken by all other agents a-posteriori. Usually, it is adopted to introduce coordination among agents and some level of global observability.

<!-- partial observability POMDP-dec -->
The rewards and information available to agents in MAS can significantly impact the complexity of the problem. In cases where all agents receive a common reward and have complete knowledge of the environment (fully observable and fully cooperative), the problem can be reduced to a single-agent problem, allowing for the identification of exact optimal policies without coordination among agents [10pp]. However, these assumptions are often not met in reality, and even if they were, it would be beneficial to factorise the joint stochastic policy into ${\pi(a \mid s) = \forall_i \pi_i (a_i \mid s)}$ to avoid the exponential growth of the action space $A$ with the number of agents, which can make greedy action selection, exploration, and learning architectures intractable. On the other hand, this inevitably leads to the problem of partial observability, where agents must act and learn with limited knowledge of the state of the world. 

Another issue with independent agents is that it is akin to having N learning algorithms running in a simulated environment that is constantly changing due to unpredictable rules, as each agent is simultaneously learning a policy to interact with the environment. This effect of non-stationarity of the environment is referred to as the moving target problem and can be formulated as $P(s' \mid s, a, \pi_1, \dots, \pi_N) \neq P(s' \mid s, a, \bar{\pi_1}, \dots, \bar{\pi_N})$, which is the change in transition probability function as a result of the co-evolution of all agents' policies.

As the focus of this thesis is on decentralised control settings under the assumption of partial observability, it is important to emphasise the difficulty of solving the decision problem for Decentralised Partially Observable Markov Decision Processes (Dec-POMDPs). In fact, computing even an approximately optimal policy for Dec-POMDPs is NEXP-complete [17,1pp]. Despite some recent empirical successes [2pp,3pp,4pp], finding an exact solution to Dec-POMDPs using Reinforcement Learning (RL) methods with theoretical guarantees is still an open research question. Nonetheless, by introducing the relaxation of free communication between agents, we can expand the knowledge of the agents and move the problem into P-SPACE [18], without introducing unrealistic abstractions that are only attainable in simulations.

## Communication and Consensus Necessities
<!-- Introduction of communication/consensus -->
In order to achieve coordination among decentralised, independent agents, some form of communication and agreement on actions must be established. This can be thought of as a distributed optimisation problem, where consensus in policy development (the development of an optimal policy attainable in multi-agent contexts) is achieved through local computation and communication with neighbouring agents.

In standard consensus algorithms, we have a set of agents $a_i$, from $A={i \in 1, 2, \dots, N}$, each initialised with some initial value. To enable communication, we can imagine the agents being interconnected over a reliable communication network, ideally represented as an oriented graph. To reach consensus, every agent communicates with one another, exchanging values, locally processes the information, and then proposes a single value $v_i$, drawn from the set $S={i \in 1,2, \dots, M}$. The agents are said to reach a consensus if, from a certain time step $t$, it holds, $\lim_{t \to \infty} v_1^t = v_2^t = \dots = v_N^t, \quad \forall i \in V$, for every set of initial values $\in S^m$.

A consensus algorithm is considered to be formally correct [19] if it satisfies the following three conditions in every execution:

- **Termination**: eventually, all `correct` processes set their decision variable.
- **Agreement**: the decision value of all `correct` processes is the same.
- **Integrity**: if all `correct` processes propose the same value, then any `correct` process in the `decided` state must choose that value.

By keeping these conditions in mind but relaxing some strict constraints like the convergence to a single state value of consensus, this research will focus on the design of a method of agreement among agents to optimise the local policy for individual needs but in accordance with the group's intentions.

<!-- byzantine generals -->
<!-- interactive consensus -->

# Common Approaches
<!-- MARL: common approaches -->
MARL is a technique used to train multiple agents to learn and interact in a shared environment. One of the main challenges of MARL is handling large environments due to the added complexity of having multiple entities interacting and the need for scalable approaches [1g,1h,1e]. One potential solution for addressing this challenge is to use imitation learning [1c], in which agents learn to imitate the actions of an expert demonstrator by using a set of collected trajectories. For instance, [1a,1aa] uses this approach to transfer the driving ability of human experts to an agent that can control a physical car, reducing the need for extensive trial and error exploration of the enormous state space that characterises autonomous driving. However, this approach requires a significant amount of expert trajectories, which, being specific to the environment and task at hand, are hardly reusable for other tasks, and there is also a potential for overfitting during training.
Another relevant method is hierarchical reinforcement learning (HRL) [1d,f]. In HRL, the learning process is divided into multiple levels or layers of abstraction, with each one being used to represent the space or the goal with a different degree of granularity. For example, an RL agent learning to play a video game might have a lower level that focuses on actions such as moving and jumping, and a higher level that rewards strategies such as exploring the environment or attacking enemies. By breaking the learning process into multiple levels, HRL can simplify the problem and make it more manageable at scale. Indeed, it is currently considered a state-of-the-art technique in the field of robotic control problems [1b]. The downside is that HRL can be difficult to implement, as it requires careful design of the hierarchical structure and abstraction at each level, with the associated risk of leading to sub-optimal solutions during training or lack of generalisation.
On the other hand, we find extensions of single agent algorithms or fully decentralised approaches that focus on learning directly from the interactions of multiple agents in the environment, allowing them to handle non-stationary and changing environments more effectively.

## Centralised and Decentralised Approaches
<!-- Centr, Decentr, CTDE -->
When dealing with simple MARL applications, we can adopt a centralised approach where the environment is viewed as a whole and the interactions between agents are observed from a global perspective. While this simplifies interactions and makes policy computation easier, it would not be suitable for the scalability of the system, a requirement that has recently attracted much attention in the development of new methods [1g,1h,1e]. Additionally, the assumptions of centralisation can be difficult to achieve in practice, as it may not reflect the reality of a scenario with a central entity in the system.
An alternative approach is decentralised control, where each agent makes its own decisions independently, without the need for a central controller or global coordination. The independent learning framework can obtain good empirical performance in several benchmarks [2a], but there are few theoretical guarantees for decentralised learning, and the interpretability is often insufficient.
Recent work has focused on a hybrid approach [2b,2c,2d,3], where global information is required only during the training phase, allowing the algorithm to be free of the need for continuous awareness of other agents' behaviour during the testing phase. Centralised Training with Decentralised Execution (CTDE) is one such approach, which has been expanded into two main lines of research that align with standard MARL frameworks. Actor-critic models, such as Multi-Agent Deep Deterministic Policy Gradient (MADDPG) [2g], use a centralised per-agent critic to estimate the Q-function and decentralised actors to optimise the agents' policies. There is no explicit communication in this approach, as the other agents' actions are inferred from their respective policies. Another similar approach is Counterfactual Multi-Agent (COMA) [2h], which addresses the challenges of multi-agent credit assignment by using a counterfactual baseline that marginalises out a single agent's action while keeping the other agents' actions fixed. However, these actor-critic models require on-policy learning, which can be sample-inefficient, especially when the state space is large.
An alternative CTDE approach is to learn a centralised Q-function [2d,3,2e,2f,3b], in which the optimality is reached by considering the relationship between joint action value and optimal local actions. For example, Value Decomposition Networks (VDN) [2d] learn the joint-action Q-values by factoring them as the sum of each agent's Q-value, and QMIX [3] extends VDN to allow the joint action Q-function to be a monotonic combination of each agent's Q-value that can vary depending on the state. Despite achieving excellent results, QMIX has faced criticism for its limited representation capacity due to the monotonic constraint, and several alternatives have been developed to address this limitation. Between the most important, we find QTRAN [2e], which learns an unrestricted joint action-value function and aims to solve a constrained optimisation problem in order to decentralise it, and QPLEX [3b], which takes advantage of the dueling network architecture to factor the joint Q function in a manner that does not restrict the representational capacity. In this thesis, I will utilise the advances of the centralised Q-function approach, but provide sufficient information for agents to act functionally in a non-stationary environment.

### Communication Channel
<!-- Communication channel -->
Mechanisms for information sharing and communication are introduced to reduce non-stationarity effects [4c,4d]. Communication between agents can take the form of explicit communication using talk channels, or implicit communication by observing other agents' actions or their effect on the environment. In the former case, one option is to rely on standardised message formats, such as the Agent Communication Language (ACL), to enable independent agents to communicate with a precisely defined syntax, semantics and pragmatics [4a17,4a19]. On the other hand, ad-hoc communication protocols with learnt communication are mostly adopted when complex coordination is required. Even though the first approach may result in a lack of generality and flexibility due to the imposed structure, having a standardised and well-defined structure is helpful when the goal necessitates common understanding. Some research tries to find a balance between standardisation and flexible communication taking the best of both approaches [4a,4b].
Efficiency, as well, is a common driver in designing effective communication, as real-world environments involve other factors such as security overhead, message brokering time and dynamism of the whole environment. There are differing opinions on the best format for messages (structured, discrete, continuous, etc.) [4b,4e,6c,2g] and the optimal method for exchanging information in terms of costs and benefits. Some methods include using a common memory buffer where agents can write and read to share information [5a], adopting an event-based framework where communication occurs only under certain circumstances [5aa], and integrating implicit communication by observing the actions of others and inferring their policies to reduce the communication needs [4l].
Communication can take place in one or both directions: direct messages are shared between two agents by opening a communication channel [4j,4i], or by broadcasting it to everyone [6a,6b,6c,6e]. The latter approach is more expressive under partial observability assumptions, but it is also more expensive in terms of transmission traffic and can lead to irrelevant information overwhelming the relevant information in crowded settings. Some approaches aim to create smaller groups of agents that focus on inter- and intra-correlation to limit irrelevant reasoning and improve coordination performance [4g,5d,5g,4i,6g,6h]. Others reduce the number of messages sent by learning to understand when communication is really necessary or when the information held is redundant and communication avoidable [5g,4l]. Some solutions aim at targeting communication to those who are interested in it [4j], while others act on the side of the listener by adopting different mechanisms of attention to filter out irrelevant messages [6e,4e,6a,6c]. While these methods of obscuration or filtering are often effective, they typically do not take into consideration the messages in the context of the whole communication channel, but are filtered only for their relevance to the individual agent, ignoring possibilities of more complex coordination.
<!--Consider the possibility that agents may act with self-interest or with adversarial intentions, as analyzed in [5b,5aside].-->
Lastly, the simplification of designing architectures that encourage cooperation has greatly facilitated research in this direction [2h,4e,4g,4h,4l,6c], in contrast to mixed environments [5b,5aside] where the lack of a rigid assumption of cooperation to succeed can make the task more challenging. This work, in particular, does not impose a fixed structure on communication, while maintaining a focus on efficiency, and does not limit its applicability to cooperative frameworks.  

### Message Content
<!-- What, when communicate and how to include the info in your choices -->
The message content is a crucial aspect for the recipient agent. It should be designed to provide additional information about the communicator's perspective, reducing uncertainty about their behaviour and facilitating coordination. One option is to use a highly expressive message encapsulating the agent's reasoning process. For instance, some works [6c,6d,6e,6f] have structured their architectures around recurrent modules, using the hidden state as a signal message for others. However, this is typically used internally to encode past and current information and reusing the individual reasoning vector for communication intent can have limiting effects in difficult tasks. Other works [6c,6d] merge multiple incoming messages into a single communication vector using weighted operations, which may not result in strong coordination when many agents participate in the communication.\\
[6e,6g] use crafted messages to transmit elaborate information to others and train successful agents able to understand each other. They also use weight sharing between agents, which is quite common [6a,6e,6f,6g] to reach a better action understanding and counteract scalability issues. Nevertheless, this implies that agents have the same reasoning ability on a piece of information, which may not be an accurate representation of the agents' individual abilities and affect the learning process of policies to be effective for the average agent, but not for each individual agent.
More straightforward transmission approaches [5g,6a] use current or time-delayed observation as a communication message, usually with additional information expressing intentions. This implementation delegates most of the interpretation and coordination to the receiver but allows for more flexible interaction dynamics. In combination with the use of a module to learn a better representation [6b], this method can also be used without the burden of raw information exchange.
Finally, in some cases we can even ignore the content of the message and, assuming full mutual knowledge of each other, the effort will only be focused on learning a policy expressing coordination. Previous works [7c,7d] included the use of a mechanism where the consensus is mandatory in order to proceed, while current methods implemented with agent learning capabilities prefer a looser agreement, aiming instead to obtain convergence in target choice and coordination in actions. For example, [7b] forms groups of agents with similar objectives to have tighter cooperation and diversity of action between teams, but the proposed architecture is not end-to-end differentiable. 
The main point of this work is that coordination should not be considered as a general requirement, but rather only in certain situations, as extensive reasoning on others' beliefs and intentions can slow convergence to a good individual policy and even be harmful.

# Approach
In this section, the proposed solution for addressing the issue of coordination in MAS will be presented. This project work utilises established algorithms and principles to make significant contributions in terms of the implementation of reasoning processes for MARL agents. The claims are supported in the next chapter by thorough testing in various scenarios and by conducting an ablation study to assess the impact of each aspect of the architecture. Additionally, the effectiveness of the algorithm will be demonstrated through testing in a complex simulated environment.

## Description
<!-- from critics to description -->
The traditional approach to addressing the problem of coordination in MARL systems is to prioritise coordination over other individual considerations. While this approach may be appropriate in scenarios where cooperation is the only way to succeed, it can prove detrimental in scenarios where agents are permitted to exhibit different behaviours to achieve their own individual goals.
An intuitive example of this is a car driver who wants to go from a starting point to their destination. If the driver were to constantly consider the intentions and actions of other drivers at each "step" of the trip, evaluating the possibility of allowing another driver to go first or taking the lead themselves, the trip would become endless. This reasoning is accentuated in large and sparsely-rewarded environments, where it is harder to extract rules and the exponential possibilities of interactions with others can lead to high uncertainty of action and slow down the convergence to a good policy or suboptimal convergence.
Another critique of this approach is the reliance on complex communication channels for establishing coordination. The primary goal of communication should be to reduce uncertainty and non-stationarity effects by providing additional information about the actions and intentions of other agents. However, many methods rely on the exchange of complex vectors that encapsulate an agent's history or personal thoughts, which can be cryptic for thirds. This can make it difficult for agents to fully understand the true intentions of the speaker in a general context. Additionally, complex environments are more challenging in terms of state evaluation, thus it is important to keep the size of the state space small by providing agents with only relevant information.
As a final thought, the proposed work does not claim a declarative and imperative agreement between agents since coordination can arise from continued interactions. In fact, since all agents are subject to the same environmental rules, they will eventually avoid self-damaging decisions and occasionally cooperate to maximise their respective reward signals.
On top of that, I propose a policy that can adopt both egoistic and altruistic behaviours, a reasoning process that takes into account information from other agents, and a simple yet effective communication channel for exchanging information. The architecture of coordination and policy modules is implemented with recurrent neural networks to maintain consistency in decisions over time and is trained using a centralised training decentralised execution paradigm. In particular, it takes the name of Coordinated QMix (CoMix) since the specific CTDE algorithm adopted. Details of each module are discussed in the following sections, and the overall architecture is depicted in figure X.

## Action Policy 
The Q network is responsible for predicting the state-action value for each action of an agent based on the information at his disposal. We can define it with the following formula: $Q_i(s_i,a_i,h_i)$ where $s_i$, $a_i$, and $h_i$ are the observation, action and hidden state of agent $i$ respectively. This would be considered sufficient for an implementation of an independent learner, however, in this setting, we incorporate communication messages as additional information that can aid the action decision-making process when available. To achieve this, we reformulate the previous definition as the sum of two terms: $Q_i = Q_{self} + Q_{coord}$. Here, $Q_{self}$ represent the selfish action intention, which, once computed, produces state-action values for the current state and updates the hidden state for the next state $h_i \rightarrow h_i'$. The second term $Q_{coord}$ takes the current updated hidden state and incoming messages filtered out by a coordination module. Furthermore, the introduction of a feature extractor is made in the proposed solution. When executed on raw observation data, it extracts meaningful information, enabling the policy network to reason within a more defined space. It is worth noting that the input processing module used has shared weights among all agents. As highlighted by [6b], this is an important implementation detail to allow all agents to reason about data extracted from the same distribution.

The final formula representing the policy network involved is the following (superscript of time $t$ is omitted for brevity):
$$Q_i(s_i,a_i,h_i,\textbf{m}_i') = Q_{self}(s_i,a_i,h_i) + Q_{coord}(a_i,h_i',\textbf{m}_i')$$
The overall network is implemented with two GRU modules and linear layers to extract the value corresponding to each action for current agent's state $s_i$, sharing only the sequential hidden state vector. The recurrence of the first is necessary to process the new observation in relation to the past, while the second has the aim of properly mixing the self-interests with the new information obtained from other agents.

## Coordinator
The coordination module is responsible for determining the relevance of other agents' communications in relation to the agent's intentions by producing a coordination mask used to filter out incoming messages. A message is defined as the communicated intention of an agent to take a certain action in order to achieve a personal objective, $\hat{a}_i=\arg\max_{a}Q_{self}(s_i,a,h_i)$, and is represented by the tuple $m_i=<s_i,\hat{a}_i>$. Consequently, we define $\textbf{m}$ as the set of incoming messages sent by $n$ agents at a certain timestep $\textbf{m}=\{m_1,\dots,m_n\}$ and $\textbf{m}_i'$ as the filtered set for agent $i$ obtained with the application of the communication mask $\textbf{c}_i$, result of the coordinator execution:

$$\textbf{z}_i=\{<m_i,m_1>, \dots ,<m_i,m_n>\}^{-<m_i,m_i>}$$
$$Coord(\textbf{z}_i)=\textbf{c}_i \text{ where } \textbf{c}_i=\{c_{i,1},\dots,c_{i,n}\}^{-c_{i,i}} \text{ and } c_{i,j}= 1 \text{ if agent } i \text{ coordinate with agent } j, 0 \text{ otherwise}$$
$$\textbf{m}_i'=\textbf{m} \odot \textbf{c}_i$$

As previously proposed in [6g], the module for this reasoning is implemented using a BiGRU layer, in order to take into account the intentions of other agents all together $\textbf{z}_i$. The individual scores produced by this layer are then used to calculate an independent probability of communication through a two-way softmax.

## Centralised Network
A learning algorithm was adopted based on the CTDE paradigm, where the state-action values and observations of the agents are provided to a centralised network to learn a joint action-value function. The specific implementation used is the one proposed by [Rashid], which decomposes the joint function into factors depending only on individual agents, enabling it to cope with large joint action spaces. Therefore, when defining $Q^{TOT}$, we have to respect the following two properties:

- $Q^{TOT}$ yields the same result as a set of individual argmax operations performed on each $Q_i$:
$\arg\max_{\textbf{a}} Q^{TOT}(\textbf{s},\textbf{a},\textbf{h},\textbf{m}') = \begin{bmatrix}\arg\max_{a_1} Q_1(s_1,a_1,h_1,\textbf{m}_1') \ \dots \ \arg\max_{a_n} Q_n(s_n,a_n,h_n,\textbf{m}_n') \end{bmatrix}$
- The relationship between $Q^{TOT}$ and each $Q_i$ is constrained to be monotonic:
$\frac{\partial Q^{TOT}}{\partial Q_i} \geq 0, \forall i \in [i,n]$

Despite its limitations in terms of representational ability due to the monotonicity constraint, which limits QMIX to suboptimal value approximations, this algorithm has long been considered a state-of-the-art approach in the field. Nonetheless, the method proposed is not bound to the use of QMIX and can be adapted to different CTDE algorithms.


## Training Supervision
The agents' Q policy network is trained end-to-end with the error propagation being able to flow between the two reasoning processes by means of the hidden state shared. The method for updating the networks' parameters $\theta^Q$ follows the common implementation also adopted by [3] for QMIX, where the error is computed using the temporal difference rule of the centralised network:
$$L(\theta^Q) = |y^{TOT} - Q^{TOT}(\textbf{s}, \textbf{q}; \theta^Q)|,$$
where $y^{TOT} = \textbf{r} + \gamma \max\limits_{\textbf{q}'}Q^{TOT}(\textbf{s}', \textbf{q}'; \theta^Q')$ and $\theta^Q$, $\theta^Q'$ are respectively the parameters of the online policy network and target policy network periodically copied from $\theta^Q$ as in standard DDQN [20].

In designing an efficient learning schema for the coordination module, different options were evaluated following the same general intuition (TODO need testing time to understand if it is worth putting that in the ablation). Since the probability of communication produced by the single agent for each other sees its effects in a modified set of values for the state-action pairs, we can effectively measure in it an improvement or decrease in action performance by assuming an optimal estimate of the state-action value from the policy. Therefore, we can define the formula of update of the coordinator's parameters as a clipped delta between the maximum state-action value obtained with filtered messages $\textbf{m}_i'$ from the predicted mask of coordination with respect to the alternative estimated value obtained from messages filtered with inverted probabilities of communication $\tilde{\textbf{m}}_i' = \textbf{m} \odot (1- Coord(\textbf{r}_i; \theta^C))$.

$$L(\theta^C)= \sum^n_{i=1} w_i \Delta Q_i = \sum^n_{i=1} w_i \max(0,\max_{a_i}Q_i(s_i,h_i,\tilde{\textbf{m}}_i',a_i) - \max_{a_i}Q_i(s_i,h_i,\textbf{m}_i',a_i))$$

Following this reasoning we can improve even further the method by doing n inferences of the Q, each one considering different messages obtained from a more targeted inversion of the probabilities of communication. Instead of computing the Q-value considering to communicate with the opposite set of agents -- with respect the one predicted -- this method uses an averaged expectation of the state action value, considering the advantage of reverting the communication probability with each single agent. While being much more expensive, this training detail allows improving the results, especially when many agents are present.  

The adoption of the QMIX approach gives us an additional element that can be used to better estimate the gain in using a mask of coordination in place of another. Being the mixer network a mapping from states to a set of weights in the hidden space used independently in linear combination with the action-state value of the respective agent, we can reuse these parameters to weight each $\Delta Q_i$. In these terms, we can make updates more precisely related to the gain given by the communication with a certain agent in a certain state. We find mention of this in the above formula with the term $w_i$.


### Training details and hyperparameters
The training of the model incorporated randomness to capture the uncertainty of the predictions and to further explore the action space. On the coordinator side, coordination mask selection involved applying Gumbel Softmax to the two decision logits, which differs from the Softmax function by adding random noise from the Gumbel distribution to the output. For exploration in action selection, an epsilon decay technique was used, with the value decreasing from 0.9 to 0.05 over 60% of the training time. To mitigate the risk of catastrophic forgetting and overfitting, a weight decay with a value of $0.0001$ was used as a regularisation term, and a soft update technique was adopted for the weights of the target network. The network sizes, learning rates and decay parameters were carefully chosen for each environment to optimise the algorithm's performance and achieve the best possible results. The learning algorithm was also found to be very sensitive to the number of recurrent steps considered during training, the value of which had to be chosen according to the task dynamics of each environment.


# Evaluation
The aim of this chapter is to evaluate the proposed approach and its effectiveness in addressing the research questions stated earlier. The evaluation process will include a comparative analysis with other relevant methodologies in the field, followed by a presentation of the results obtained through a comprehensive and systematic evaluation process. The results will be examined in detail, including an examination of the proposed components and the implementation details, in order to identify overall strengths and limitations. In summary, this evaluation aims to shed light on the performance of the proposed methodology and enable the researcher to draw informed and insightful conclusions on its effectiveness in practical applications.


### Baselines of comparison
- Individualised Controlled Continuous Communication Model (IC3Net)[6d] is proposed as an extension of a previous method [6c], introducing a gating mechanism on each other agent's communication channel. They use the current observation encoded, both as internal thought and communication message, and a mask of communication to filter out incoming messages computed at the previous step. The reasoning process then occurs by processing the individual thoughts in an LSTM module and averaging the remaining incoming messages. The result is used as input to two output heads to obtain the action values and the individualised probability of communication.
<!--a message-generation network is defined at each agent and connected to other agentsâ€™ policies or critic networks through communication channels. Then, the message-generation network is trained by using the gradient of other agentsâ€™ policy or critic losses.-->
- ATOC[6g] is an attentional communication model proposed to learn effective and efficient communication at scale by adopting weight sharing between each agent's network. In their proposal, the communication message is represented by the hidden state of a recurrent module that processes the observation at each step, thus sharing a vector which resembles the agent's history. Based on the internal reasoning, the agent will decide whether or not to communicate with its neighbours, observing that this decision will create a group of maximum $m$ agents maintained for $T$ steps, with $m$ and $T$ as hyperparameters. The incoming messages from each group are mixed with a bidirectional LSTM module, where the final output is merged with the internal thoughts and processed to obtain the action values.
<!--ATOC focus on how to improve the coordination by not using simple observations as message but hidden states and using a bidirectional layer mix those for the agents partecipating in a group of communication. The groups of communication are enforced for T step to maintain recurrence over the decisions and for stability of communication.-->


//TODO aggiungi nota onpolicy
//TODO nella didascalia aggiungi nota che indica log per update

Result X1
The graph depicts on the Y axis the total rewards (sum of individual rewards obtained by the agents), delivered by the environment to the agents, against the number of updates to the weights of the network. IC3Net, learning the policy while acting, adopt a much more deterministic behaviour which result as succesfull already from first thousands iterations. CoMix learn over time a succesful strategy, while ATOC struggle since its limitation in the archtitecture. It's also worth mentioning that since the difference in training method, with a same number of updates we have the number of steps seen by ATOC being 2 scale of magnitude bigger than competitors.    

### Environments
For the evaluation, two testing environments were considered. The first of these is the **Switch** environment, a grid world where four agents must navigate to reach their respective switch on the map, having as input only the current position and target distance. The agents are faced with a challenge due to the map's layout, which features two rooms with starting points and a narrow hallway connecting them. However, the corridor can only be crossed by one agent at a time, meaning that coordinated behaviour is necessary to prevent the agents from getting stuck. It is worth mentioning that the agents are unable to see each other, and must therefore use a communication channel to gather information about the positions and intentions of their peers. The task ends after 250 steps of execution, or when all agents have successfully reached their destinations. 

The second testing environment is an instance of the **Predator-Prey** problem provided by the PettingZoo library, which implementation requires particularly high levels of coordination to succeed. The environment consists of a grid world in which some agents chase randomly moving entities with the aim of capturing them. The pursuers have limited visibility of the world, extending up to three units in each direction from their positions, and same speed as the evaders. While an individual agent can only earn a small reward for "tagging" an evader (i.e., being in its same position), a group of organised agents can receive a larger shared reward "capturing" it (i.e., surround its position in the 4 axis). The task is considered complete either when all evaders have been captured or after 500 steps of execution.


### Results
In evaluating the efficacy of the proposed method in comparison to the baselines, it is important to account for the disparity in the training methodologies employed. Specifically, IC3Net utilizes an on-policy training approach, in contrast to CoMix and ATOC. Furthermore, the latter two are trained with epsilon decay action selection, which allows for a more thorough exploration of the action space, but also leads to greater variance in the interactions with the environment.

- SWITCH
Figure X displays the learning curve of the CoMix method and the two baseline approaches in terms of the total reward received by the agents in an episode. A higher value indicates not only the agents' capability to reach their respective targets, but also their ability to do so in a minimal number of steps, since the reward decays from 10 to 5 during the agents' lifespan.\\
Given the simplicity of the task and its low potential for misunderstandings between agents, IC3Net's straightforward communication mechanism proves to be the most effective among the three methods compared. After about one thousand updates, the agents have already learnt almost deterministic behaviour and consistently obtain optimal rewards. CoMix follows, with slightly lower results, while ATOC struggles with the task. A deeper analysis of the latter reveals that in the execution, all agents tend to adopt the same action selection policy, moving in the same room. This usually ensures that at least two agents reach their intended target, while the others rarely show any further intention of turning back in the right direction. The behaviour is likely caused by weight sharing used for all components, which was not even mitigated by the introduction of an ID in the policy computation. On the other hand, IC3Net reaches optimal performance, likely because the task resolution requires low variability and therefore does not necessitate strong communication skills. If the task becomes too easy, the policy can adopt a deterministic action sequence, and the communication channel can become secondary. The CoMix method of communication and coordination maintains variability in the choices, which often leads to slower resolution but still enables all agents to reach their targets with high scores.


- PURSUIT
There, the agents will be asked to decide if act in solo operations by chasing an evader for the small "tag" reward or better coordinate in groups to "capture" them. The environments has been choosen also for the possibility of easily test the methods at scale. 3 configurations were considered with this aim:
- (a) 4 agents, 16x16 env size
- (b) 8 agents, 18x18 env size
- (c) 16 agents, 20x20 env size
While scaling the space and the number of pursuer, the number of evaders and number of steps limiting the episode length remained constant at 16 and 500 respectively to ensure comparability between results and  better compare abilities of the agents. Indeed, being the number of steps limited, and the map where evaders can hide/escape quite big, it is prevedible that not all evaders will be captured nevertheless we can observe interesting group strategies of stalking and tracking. Figure X show in the different configurations the results by each method.
We see IC3Net being very good at handling lower numbers of agents while demonstrating difficulties in obtaining decent results when we scale up the setting. We find a reson in the behaviour which is being adopted by all agents in a very coordinated manner: often they all converge in a fixed corner of the map they being all grouped there, when an evader randomly move in thyir observability area they fastly surround it. Over time this come out as a very good strategy in (a), while it does fail in (b,c) due to the sparser map and inability by the method in effectively coordinate agents in groups to cover larger portions of it. Coordinated planning is a very necessity in such bigger space. ATOC on the other hand using a more purposefull policy to consider others thoughts, demonstrate behaviours of action very committed to the task but occasionally the agents get "distracted" and move away from the group. This is also the reason of the variance in the results reported. Overtime this desire of exploration from agents disappear and they get better at the task but still agents do not show enough ability in caputuring when more agents are available.
The propoed method instead show very good abilities by agents, in maintaining groups and move in coordination towards the near evaders. Especially in the smaller case of 4 agents most of the step are spent in searching each other in the map, since no coordinate is provided to the agents, therefore observing both low and higher values of success mostly dependently by the random position of initialization. Also it seems they adopt a strategy to move all together but staying near the border (since in this case they necessitate less agents to surround) and they have more probabilities to capture.  
### TODO when you evaluate baseline also for comix8 and comix16
<!--not only shows a very good learning curve but also show capabilities of being able to coordinate succesfully the agents at different environment scales. (are results better c respect to b respect to a?)
this demonstrate practical ability in (abstract abilities?)-->



Captions

<!-- explanation metric, difference of training, environment, how many experiments per test/method -->

<!-- in ablation average over n runs average over last n steps -->


### Ablation study
<!-- We conclude our evaluation by measuring the impact of each component in our pipeline through an ablation study carried out on the two environments -->
<!-- Based on the results obtained, we conducted a detailed analysis of the performance of each coordination strategy. -->

Building upon the quantitative and qualitative analysis presented in the comparison against other methods, this section delves deeper into the factors that contribute to the success of the proposed method. Tests were repeated in both environments -- only in the smaller version in the case of Predator-Prey, to limit the use of computational resources -- considering the final proposal as a baseline against variants that differ in the architecture or training methodologies.

- \textbf{'\textit{no comm}'} ($Q$ w/o $Q_{coord}$), uses the base structure of learning, but with agents lacking communication abilities. Agents are required to understand the environment dynamics thoroughly to achieve their objective, as centralized training is the only mechanism for information sharing.
- \textbf{'\textit{no weights}'} ($L_C$ w/o $\textbf{w}$), does not use the weights provided by the QMIX framework for the computation of the Coordinator loss. This variant shows the performance of the base method if extrapolated by the current CTDE framework.
- \textbf{'\textit{true}'} ($\tilde{\textbf{c}}^{true}$) and \textbf{'\textit{inverse}'} ($\tilde{\textbf{c}}^{inverse}$) adopt different flavours of training for the Coordinator. The first use an all-true mask of coordination, and the second use a single-inference full inversion of probabilities, as explained in \ref{loss}.

#### Analysis
Figure \ref{fig:ablation-res} puts in perspective the average results obtained after an equivalent time of training for each variant discussed. As in the method evaluation, the total reward is considered as a metric of success for the Switch environment and the number of captures in Predator-Prey. The comparison also provides very interesting insights into how different choices influence agent training and consequently the strategies developed.\\
In the Switch environment, we can see 'inverse' and 'no weights' demonstrating the same coordination performances as the baseline. However, they require more training steps to reach the same achievements as they obtain lower rewards over time. In both variants we can identify situations in which the agents struggle to reach their position, but this is mainly due to individual misbehaviour rather than coordination impediments. On the other hand, we observe 'true' obtaining very good results even if with a slower convergence and a weaker coordination (See Fig. \ref{fig:ablation-comm}). Interestingly, while the baseline implementation allows agents to learn incrementally and navigate the environment, 'true' does not initially report successes. This is due to the fact that this method of supervision incentivises considering everyone's intentions instead of limiting the space for collaboration to local coordination.\\
<!-- (add example of agents not able to agree on who should pass first and this hinder the final result) -->
<!-- The 'no comm' strategy results highly problematic, with agents unable to coordinate effectively, even when placed in front of each other. -- 'no comm' shows an evident lack of coordination since the results obtained.
-->
In the Predator-Prey environment, the baseline outperformed the other variants, demonstrating superior performance and strategies not observed in others. For instance, the agents employed a group exploration strategy of moving along the map edges, as capturing prey in these positions requires fewer predators. The 'inverse' and 'true' flavours ranked behind the baseline with similar results in terms of performance. However, 'true' exhibited coordination issues and achieved few captures even when opportunities were present, while 'inverse' as well as 'no weights' encountered significant exploration difficulties.\\
<!-- (In particular, 'no weights' shows agents who tend to stay close together in the corner of the map with little interest in exploration.) -->
<!-- The 'no comm' strategy exhibited a lack of coordination, with agents spreading out instead of staying together. -->
The absence of communication capabilities results in an apparent lack of coordination between agents in both environments. This was evidenced by the tendency of the 'no comm' agents to disperse and not remain cohesive, leading to poor performance compared to the other variants. In the Predator-Prey environment, agents are often unsuccessful, indicating a failure to coordinate properly. In the Switch environment, agents seem disturbed by each other's actions, moving back and forth several times when facing one another, unable to anticipate or understand their movements.\\
To further analyse the performance of the alternatives, we examine the Coordinator module's training loss. 'no weights' exhibited high spikes in training loss, while the 'true' and 'inverse' variants showed more stable learning. However, it's possible that the stability is due to the ineffectiveness of the methods, as both 'true' and 'inverse' performed lower than the baseline and 'no weights'.\\
Looking at the results as a whole, it can be deduced that enabling communication between agents with the inclusion of the additional term in the state-action evaluation is crucial for improved performance. Regarding the tested learning modalities, it can be observed that, except for 'true', which is highly situation and environment-dependent, the others can be considered simplified variants of the baseline leading to slower results.

### Communication
To conclude our analysis, we investigated the communication mechanism of CoMix by analyzing the evolution of the predicted communication masks. Coordination success was determined by comparing the predicted state-action values against the values obtained with the alternative coordination mask when computing the Coordinator's loss. The "Good/Bad Coordination Ratio", shown in Figure \ref{fig:ablation-comm}, indicates the percentage of agents making correct predictions per each step. The increasing metric for all ablated strategies shows promising results, with the base method achieving the maximum value. However, although the proposed strategy is generally applicable, adopting a learning process for the Coordinator tailored to the specific environment dynamics could potentially yield better results (e.g., in a fully cooperative environment, could be more proficient training against the maximum amount of information and then learn to filter out what is perceived as irrelevant for the current step). Another important finding is given by the number of times agents coordinate with others during training. In the Switch environment, we observe a decrease in this value, whereas in the Predator-Prey environment, the value remains almost constant and higher overall. This finding aligns with the intuition, as the latter environment requires all four agents to coordinate in order to capture prey.
<!-- TODO should i put the graph? Should I divide by ablation flavours? the description is valid for optout and slightly also for no weights -->
Furthermore, it should be noted that the CoMix implementation inherently offers interpretability for the agents' decisions on actions. We can trace an action back to single interactions with other agents or to the agents' self-imposed objectives. For example, in the Switch environment, when a single agent remains, $Q_{coord}$ term has a null value since its actions are not influenced by others. In the case of the Predator-Prey environment, we can observe the norm of $Q_{self}$ and $Q_{coord}$ to determine whether an agent acts primarily by following its own will or by adopting a strategy aimed at coordination.